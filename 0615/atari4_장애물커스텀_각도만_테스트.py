import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
import time
from collections import deque

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class GridEnvironment:
    def __init__(self, grid_size=(7, 22)):
        self.grid_size = grid_size
        self.grid = np.zeros(grid_size)
        self.grid[0, :] = 1 # ìƒë‹¨ ë²½
        self.grid[-1, :] = 1        # í•˜ë‹¨ ë²½
        self.grid[:, 0] = 1       # ì¢Œì¸¡ ë²½
        self.grid[:, -1] = 1    # ìš°ì¸¡ ë²½

        self.agent_pos = [3, 1] # ì‹œì‘ ìœ„ì¹˜
        self.start_pos = list(self.agent_pos)
        self.crashed = False
        self.current_steps = 0
        self.previous_distance = 0 # ì´ì „ ìŠ¤í…ì—ì„œì˜ ê±°ë¦¬.

        self.backstep = 0 # ë’¤ë¡œ ê°„ íšŸìˆ˜

        self.heading = 0  # ê°ë„ë„ (0ë„) 0ë„ëŠ” ì˜¤ë¥¸ìª½, 90ë„ëŠ” ìœ„ìª½, 180ë„ëŠ” ì™¼ìª½, 270ë„ëŠ” ì•„ë˜ìª½
        
        # ì‹œê°í™”ë¥¼ ìœ„í•œ ì†ì„± ì¶”ê°€
        self.position = self.agent_pos
        self.start = self.start_pos
        self.prevPosition = []
        self.small_goal_range = [(1, 20), (5, 20)]
        self.direction_map = {
            0:   (0, 1),     # â†’
            45:  (-1, 1),    # â†— (ìœ„ë¡œ í•œ ì¹¸, ì˜¤ë¥¸ìª½ í•œ ì¹¸)
            90:  (-1, 0),    # â†‘
            135: (-1, -1),   # â†–
            180: (0, -1),    # â†
            225: (1, -1),    # â†™
            270: (1, 0),     # â†“
            315: (1, 1)      # â†˜
        }

    def reset(self):
        success = self.generate_valid_map()
        if not success:
            print("âš ï¸ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨ - ì˜ˆë¹„ ë§µ ì‚¬ìš©ë¨")

        self.agent_pos = list(self.start_pos)
        self.position = self.agent_pos
        self.heading = 0
        self.crashed = False
        self.current_steps = 0
        self.prevPosition = []
        self.backstep = 0

        return self.get_state()

    def place_random_obstacles(self, count=30, avoid_area=None): # ì¥ì• ë¬¼ ë°°ì¹˜ í•¨ìˆ˜ ì¥ì• ë¬¼ì€ ëœë¤ìœ¼ë¡œ
        placed = 0
        avoid_set = set(avoid_area or [])
        while placed < count:
            x = random.randint(1, self.grid_size[0] - 2)
            y = random.randint(1, self.grid_size[1] - 2)
            if self.grid[x, y] == 0 and (x, y) not in avoid_set:
                self.grid[x, y] = 1
                placed += 1

    def is_path_exists(self, grid, start, goal_cells): # í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ ì´ë™
        """ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (BFS ì‚¬ìš©)"""
        visited = set()
        queue = deque([tuple(start)])
        visited.add(tuple(start))

        while queue:
            x, y = queue.popleft()
            if (x, y) in goal_cells:
                return True
            
            for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:
                nx, ny = x + dx, y + dy
                if (0 <= nx < grid.shape[0] and 0 <= ny < grid.shape[1] and 
                    grid[nx, ny] == 0 and (nx, ny) not in visited):
                    visited.add((nx, ny))
                    queue.append((nx, ny))
        return False

    def generate_valid_map(self, max_trials=20, obstacle_count_range=(10, 40)):
        for trial in range(max_trials):
            # ê·¸ë¦¬ë“œ ì´ˆê¸°í™”
            self.grid = np.zeros(self.grid_size)
            self.grid[0, :] = 1  # ìƒí•˜ì¢Œìš° ë²½
            self.grid[-1, :] = 1
            self.grid[:, 0] = 1
            self.grid[:, -1] = 1

            # í”¼í•´ì•¼ í•  ìœ„ì¹˜ë“¤ (ì‹œì‘ì  + ëª©í‘œì§€ì  ì „ì²´)
            avoid = [tuple(self.start_pos)] + [(x, 20) for x in range(1, 6)]

            # ì¥ì• ë¬¼ ë°°ì¹˜
            count = random.randint(*obstacle_count_range)
            self.place_random_obstacles(count=count, avoid_area=avoid)

            # ê²½ë¡œ ì¡´ì¬ í™•ì¸
            goal_cells = [(x, 20) for x in range(1, 6)]
            if self.is_path_exists(self.grid, self.start_pos, goal_cells):
                #print(f"âœ… ìœ íš¨í•œ ë§µ ìƒì„±ë¨ (ì‹œë„ {trial + 1}/{max_trials}, ì¥ì• ë¬¼ {count}ê°œ)")
                return True
        
        print(f"âŒ {max_trials}ë²ˆ ì‹œë„ í›„ ìœ íš¨í•œ ë§µ ìƒì„± ì‹¤íŒ¨")
        return False

    def dist_to_goal(self, x, y):
        # ëª©í‘œ ì§€ì ê³¼ì˜ ê±°ë¦¬ ê³„ì‚° (ì˜ˆ: ìœ í´ë¦¬ë“œ ê±°ë¦¬)
        # ì‘ì€ ëª©í‘œ ì§€ì  (1, 20) ~ (5, 20) ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°
        min_dist = float('inf')
        for gx in range(self.small_goal_range[0][0], self.small_goal_range[1][0] + 1):
            gy = self.small_goal_range[0][1]  # yëŠ” ê³ ì •: 20
            dist = np.sqrt((x - gx) ** 2 + (y - gy) ** 2)
            if dist < min_dist:
                min_dist = dist
        return min_dist
    
    def get_sensor_distances(self, x, y, heading):
        """ì„¼ì„œê°€ ë°”ë¼ë³´ëŠ” 3ë°©í–¥: -45ë„, 0ë„, +45ë„ì˜ ê±°ë¦¬ ë°˜í™˜"""
        angles = [
                (heading - 90) % 360,
                (heading - 45) % 360,
                heading % 360,
                (heading + 45) % 360,
                (heading + 90) % 360
        ]
        
        distances = []
        for angle in angles:
            angle = int(angle) % 360
            angle = (round(angle / 45) * 45) % 360  # 45ë„ë¡œ ë¼ìš´ë”© í›„ ì •ê·œí™”
            
            if angle not in self.direction_map:
                angle = 0  # ê¸°ë³¸ê°’
                
            dx, dy = self.direction_map[angle]
            dist = 0
            nx, ny = x, y

            # ë²½(1)ì„ ë§Œë‚  ë•Œê¹Œì§€ ì´ë™
            while True:
                nx += dx
                ny += dy
                dist += 1
                if (nx < 0 or nx >= self.grid_size[0] or 
                    ny < 0 or ny >= self.grid_size[1] or
                    self.grid[nx, ny] == 1):
                    break
            distances.append(dist)

        return distances  # [ì™¼ìª½(-45ë„), ì •ë©´(0ë„), ì˜¤ë¥¸ìª½(+45ë„)]
    
    def is_in_goal(self, x, y):
        goal_x_min = self.small_goal_range[0][0]
        goal_x_max = self.small_goal_range[1][0]
        goal_y = self.small_goal_range[0][1]
        return goal_x_min <= x <= goal_x_max and y == goal_y    
    
    def step(self, action):
        self.current_steps += 1
        x, y = self.agent_pos
        
        reward = -0.1

        # ì´ë™ ë¡œì§
        if action == 0:  # +45ë„ íšŒì „
            self.heading = (self.heading + 45) % 360
        elif action == 1:  # ì „ì§„
            # heading ê°’ì„ 45ë„ ê°„ê²©ìœ¼ë¡œ ì •ê·œí™”
            normalized_heading = (round(self.heading / 45) * 45) % 360
            if normalized_heading not in self.direction_map:
                normalized_heading = 0  # ê¸°ë³¸ê°’ ì„¤ì •
                
            dx, dy = self.direction_map[normalized_heading]
            new_x = x + dx
            new_y = y + dy
            
            # ë²”ìœ„ ì²´í¬ ì¶”ê°€
            if (0 <= new_x < self.grid_size[0] and 
                0 <= new_y < self.grid_size[1] and 
                self.grid[new_x, new_y] == 0):
                x, y = new_x, new_y
            else:
                self.crashed = True
        elif action == 2:  # -45ë„ íšŒì „
            self.heading = (self.heading - 45) % 360
        
        self.agent_pos = [x, y]
        self.position = list(self.agent_pos)

        

        # ì¶©ëŒ ì‹œ ì²˜ë¦¬
        if self.crashed:
            return self.get_state(), -10, True, {}
        
        '''# ìµœê·¼ ë°©ë¬¸ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ ìœ ì§€)
        if len(self.prevPosition) >= 8:
            self.prevPosition.pop(0)
        self.prevPosition.append((x, y))

        # ìµœê·¼ 5ë²ˆ ì¤‘ì— í•´ë‹¹ ìœ„ì¹˜ê°€ 5ë²ˆ ì´ìƒ ë“±ì¥í•˜ë©´ ì¢…ë£Œ
        if self.prevPosition.count((x, y)) >= 5:
            return self.get_state(), -5, True, {}
'''
        reward = 1

        sensor_distances = self.get_sensor_distances(x, y, self.heading)
        front_distance = sensor_distances[1]    # ì •ë©´ ê±°ë¦¬

        if front_distance < 2:
            reward -= 2

        # ëª©í‘œ ì§€ì ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸   
        if self.is_in_goal(x, y):
            reward = 30
            global goal_reached
            #goal_reached += 1
            return self.get_state(), reward, True, {}

        if self.current_steps > 100: # ë„ë‹¬í•˜ì§€ ëª»í–ˆì„ ë•Œ
            return self.get_state(), -30, True, {}

        return self.get_state(), reward, False, {}
    
    def get_state(self):
        # ì„¼ì„œ ê±°ë¦¬ë§Œ ë°˜í™˜ (3ê°œ ê°’)
        state = list(self.get_sensor_distances(self.agent_pos[0], self.agent_pos[1], self.heading))
        return np.array(state, dtype=np.float32)

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.fc(x)
 
def visualize_episode_steps(env, model, episode_num, fig=None, ax=None):
    """í•œ ì—í”¼ì†Œë“œì˜ ìŠ¤í…ë³„ ì´ë™ ê³¼ì •ì„ ì‹œê°í™”"""
    state = env.reset()
    done = False
    path = [env.agent_pos.copy()]  # ì‹œì‘ ìœ„ì¹˜ ì €ì¥
    rewards = []
    headings = [env.heading]  # ì‹œì‘ heading ì €ì¥

    # ì—í”¼ì†Œë“œ ì „ì²´ ê²½ë¡œ ë¯¸ë¦¬ ê³„ì‚°
    while not done and env.current_steps < 100:  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = model(state_tensor)
        action = q_values.argmax().item()
        next_state, reward, done, _ = env.step(action)

        path.append(env.agent_pos.copy())
        headings.append(env.heading)  # âœ… ìŠ¤í…ë§ˆë‹¤ heading ì €ì¥
        rewards.append(reward)
        state = next_state

    # ê° ìŠ¤í…ë³„ë¡œ ì‹œê°í™”
    for step in range(len(path)):
        ax[0].clear()  # ì²« ë²ˆì§¸ subplotë§Œ ì´ˆê¸°í™”

        # ê·¸ë¦¬ë“œ ì´ˆê¸°í™”
        grid_display = np.zeros(env.grid_size)
        for i in range(env.grid_size[0]):
            for j in range(env.grid_size[1]):
                if env.grid[i, j] == 1:
                    grid_display[i, j] = 0.7  # ë²½/ì¥ì• ë¬¼

        # ìœ„í—˜ ì˜ì—­ í‘œì‹œ
        x1, y1 = env.small_goal_range[0]
        x2, y2 = env.small_goal_range[1]
        grid_display[x1:x2+1, y1:y2+1] = 0.3
        ax[0].imshow(grid_display, cmap='Greys', alpha=0.5)

        # í˜„ì¬ê¹Œì§€ì˜ ê²½ë¡œ í‘œì‹œ
        if step > 0:
            path_coords = np.array(path[:step+1])
            ax[0].plot(path_coords[:, 1], path_coords[:, 0], 'b-', alpha=0.7)

            # ì´ë™ ê²½ë¡œìƒì˜ ëª¨ë“  ì  í‘œì‹œ
            for idx, (pos_x, pos_y) in enumerate(path[:step]):
                ax[0].plot(pos_y, pos_x, 'bo', alpha=0.3, markersize=8)

        # ì‹œì‘ì ê³¼ í˜„ì¬ ìœ„ì¹˜ í‘œì‹œ
        ax[0].plot(path[0][1], path[0][0], 'go', markersize=15, label='Start')
        current_x, current_y = path[step]
        ax[0].plot(current_y, current_x, 'r*', markersize=15, label='Current')

        # ğŸ”„ í˜„ì¬ heading ë°©í–¥ í‘œì‹œ (ìŠ¤í…ì— ë§ê²Œ)
        heading = headings[step] if step < len(headings) else env.heading
        dx, dy = env.direction_map[heading]
        ax[0].arrow(current_y, current_x, dy * 0.5, dx * 0.5,
                    head_width=0.3, head_length=0.3, fc='red', ec='red')

        # ê²©ì ê·¸ë¦¬ê¸°
        for i in range(env.grid_size[0]):
            for j in range(env.grid_size[1]):
                ax[0].axhline(y=i+0.5, color='black', linewidth=0.5, alpha=0.3)
                ax[0].axvline(x=j+0.5, color='black', linewidth=0.5, alpha=0.3)

        ax[0].grid(False)
        ax[0].set_xticks(range(env.grid_size[1]))
        ax[0].set_yticks(range(env.grid_size[0]))

        # í˜„ì¬ê¹Œì§€ì˜ ëˆ„ì  ë³´ìƒ ê³„ì‚°
        cumulative_reward = sum(rewards[:step]) if step > 0 else 0
        ax[0].set_title(f'Episode {episode_num+1}, Step {step}/{len(path)-1}\n'
                        f'Position: {path[step]}, Heading: {heading}Â°, Reward: {cumulative_reward:.1f}')
        ax[0].legend()

        plt.draw()
        plt.pause(0.05)  # ì§€ì—° ì‹œê°„ ê°ì†Œ

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    return path, rewards  # ê²½ë¡œì™€ ë³´ìƒ ë°˜í™˜

# ì¥ì¹˜ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# í™˜ê²½ ë° ëª¨ë¸ ì´ˆê¸°í™”
env = GridEnvironment()
state_dim = len(env.get_state())  # ìƒíƒœ ì°¨ì›
action_dim = 3  # 0: ìš°íšŒì „, 1: ì „ì§„, 2: ì¢ŒíšŒì „

# ëª¨ë¸ ì •ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë”©
model = DQN(state_dim, action_dim).to(device)
model.load_state_dict(torch.load("ì§ì„ ìœ¼ë¡œì´ë™_ì¥ì• ë¬¼ë„_ì„¼ì„œê°’ë§Œì…ë ¥.pth", map_location=device))
model.eval()

# ì‹œê°í™”ìš© Figure ì´ˆê¸°í™”
plt.ion()
fig, ax = plt.subplots(1, 2, figsize=(16, 8))  # [0]: ë§µ, [1]: ë³´ìƒ ê·¸ë˜í”„

# í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° ì‹œê°í™”
for episode in range(5):
    path, rewards = visualize_episode_steps(env, model, episode_num=episode, fig=fig, ax=ax)
    print(f"[Test Episode {episode + 1}] ì´ ìŠ¤í…: {len(path)} | ì´ ë³´ìƒ: {sum(rewards):.2f}")
    plt.pause(1.0)  # ì—í”¼ì†Œë“œ ê°„ pause

plt.ioff()
plt.show()
